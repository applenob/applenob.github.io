<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/favicon.ico" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"applenob.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="基础介绍首先看下模型结构，对模型有一个直观的概念：  描述下这个图：分成两排，第一排是$y$序列，第二排是$x$序列。每个$x$都只有一个$y$指向它，每个$y$也都有另一个$y$指向它。 OK，直觉上的东西说完了，下面给出定义(参考《统计学习方法》)：">
<meta property="og:type" content="article">
<meta property="og:title" content="隐马尔科夫模型（HMM）及其Python实现">
<meta property="og:url" content="https://applenob.github.io/machine_learning/HMM/index.html">
<meta property="og:site_name" content="Javen Chen&#39;s Blog">
<meta property="og:description" content="基础介绍首先看下模型结构，对模型有一个直观的概念：  描述下这个图：分成两排，第一排是$y$序列，第二排是$x$序列。每个$x$都只有一个$y$指向它，每个$y$也都有另一个$y$指向它。 OK，直觉上的东西说完了，下面给出定义(参考《统计学习方法》)：">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://applenob.github.io/machine_learning/HMM/hmm.png">
<meta property="og:image" content="https://applenob.github.io/machine_learning/HMM/hmm.jpg">
<meta property="og:image" content="https://applenob.github.io/machine_learning/HMM/forward.png">
<meta property="article:published_time" content="2016-12-15T19:00:00.000Z">
<meta property="article:modified_time" content="2024-11-10T20:30:54.078Z">
<meta property="article:author" content="Javen Chen">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://applenob.github.io/machine_learning/HMM/hmm.png">


<link rel="canonical" href="https://applenob.github.io/machine_learning/HMM/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://applenob.github.io/machine_learning/HMM/","path":"machine_learning/HMM/","title":"隐马尔科夫模型（HMM）及其Python实现"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>隐马尔科夫模型（HMM）及其Python实现 | Javen Chen's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Javen Chen's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Tech and Life~</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.</span> <span class="nav-text">基础介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BD%A2%E5%BC%8F%E5%AE%9A%E4%B9%89"><span class="nav-number">1.1.</span> <span class="nav-text">形式定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E4%B8%AA%E5%9F%BA%E6%9C%AC%E5%81%87%E8%AE%BE"><span class="nav-number">1.2.</span> <span class="nav-text">隐马尔科夫模型的两个基本假设</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E4%B8%AA%E5%85%B3%E4%BA%8E%E6%84%9F%E5%86%92%E7%9A%84%E5%AE%9E%E4%BE%8B"><span class="nav-number">1.3.</span> <span class="nav-text">一个关于感冒的实例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-HMM%E7%9A%84%E4%B8%89%E4%B8%AA%E9%97%AE%E9%A2%98"><span class="nav-number">2.</span> <span class="nav-text">2.HMM的三个问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E6%A6%82%E7%8E%87%E8%AE%A1%E7%AE%97%E9%97%AE%E9%A2%98"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 概率计算问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98"><span class="nav-number">2.2.</span> <span class="nav-text">2.2学习问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3%E9%A2%84%E6%B5%8B%E9%97%AE%E9%A2%98"><span class="nav-number">2.3.</span> <span class="nav-text">2.3预测问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="nav-number">3.</span> <span class="nav-text">3.完整代码</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Javen Chen"
      src="/images/ggb.png">
  <p class="site-author-name" itemprop="name">Javen Chen</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">115</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/applenob" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;applenob" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:applenobcer@gmail.com" title="E-Mail → mailto:applenobcer@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-Hans" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://applenob.github.io/machine_learning/HMM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/ggb.png">
      <meta itemprop="name" content="Javen Chen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Javen Chen's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="隐马尔科夫模型（HMM）及其Python实现 | Javen Chen's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          隐马尔科夫模型（HMM）及其Python实现
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2016-12-15 11:00:00" itemprop="dateCreated datePublished" datetime="2016-12-15T11:00:00-08:00">2016-12-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-11-10 12:30:54" itemprop="dateModified" datetime="2024-11-10T12:30:54-08:00">2024-11-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="基础介绍"><a href="#基础介绍" class="headerlink" title="基础介绍"></a>基础介绍</h2><p>首先看下模型结构，对模型有一个直观的概念：</p>
<p><img src="/machine_learning/HMM/hmm.png"></p>
<p>描述下这个图：<br>分成两排，第一排是$y$序列，第二排是$x$序列。每个$x$都只有一个$y$指向它，每个$y$也都有另一个$y$指向它。</p>
<p>OK，直觉上的东西说完了，下面给出定义(参考《统计学习方法》)：</p>
<ul>
<li><strong>状态序列（上图中的$y$，下面的$I$）</strong>：<br>隐藏的马尔科夫链随机生成的状态序列，称为状态序列（state sequence）</li>
<li><strong>观测序列（上图中的$x$，下面的$O$）</strong>:<br>每个状态生成一个观测，而由此产生的观测的随机序列，称为观测序列（obeservation sequence）</li>
<li><strong>马尔科夫模型</strong>：<br>马尔科夫模型是关于时序的概率模型，描述由一个隐藏的马尔科夫链<strong>随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列</strong>的过程。</li>
</ul>
<h3 id="形式定义"><a href="#形式定义" class="headerlink" title="形式定义"></a>形式定义</h3><p>设$Q$是所有可能的状态的集合，$V$是所有可能的观测的集合。</p>
<p>$Q&#x3D;{q_1,q_2,…,q_N},V&#x3D;{v_1,v_2,…,v_M}$<br>其中，$N$是可能的状态数，$M$是可能的观测数。</p>
<p>$I$是长度为$T$的状态序列，$O$是对应的观测序列。<br>$I&#x3D;(i_1,i_2,…,i_T),O&#x3D;(o_1,o_2,…,o_T)$</p>
<p><strong>A是状态转移矩阵</strong>：$A&#x3D;[a_{ij}]_{N×N}$ $i&#x3D;1,2,…,N; j&#x3D;1,2,…,N$</p>
<p>其中，在时刻$t$，处于$q_i$ 状态的条件下在时刻$t+1$转移到状态$q_j$ 的概率：<br>$a_{ij}&#x3D;P(i_{t+1}&#x3D;q_j|i_t&#x3D;q_i)$</p>
<p><strong>B是观测概率矩阵</strong>：$B&#x3D;[b_j(k)]_{N×M}$ $k&#x3D;1,2,…,M; j&#x3D;1,2,…,N$</p>
<p>其中，在时刻$t$处于状态$q_j$ 的条件下生成观测$v_k$ 的概率：<br>$b_j(k)&#x3D;P(o_t&#x3D;v_k|i_t&#x3D;q_j)$</p>
<p><strong>π是初始状态概率向量</strong>：$π&#x3D;(π_i)$<br>其中，$π_i&#x3D;P(i_1&#x3D;q_i)$</p>
<p>隐马尔科夫模型由初始状态概率向量$π$、状态转移概率矩阵A和观测概率矩阵$B$决定。$π$和$A$决定状态序列，$B$决定观测序列。因此，隐马尔科夫模型$λ$可以由三元符号表示，即：$λ&#x3D;(A,B,π)$。$A,B,π$称为隐马尔科夫模型的<strong>三要素</strong>。</p>
<h3 id="隐马尔科夫模型的两个基本假设"><a href="#隐马尔科夫模型的两个基本假设" class="headerlink" title="隐马尔科夫模型的两个基本假设"></a>隐马尔科夫模型的两个基本假设</h3><ol>
<li>设隐马尔科夫链在任意时刻$t$的状态只依赖于<strong>其前一时刻</strong>的状态，与其他时刻的状态及观测无关，也与时刻$t$无关。（<strong>齐次马尔科夫性假设</strong>）</li>
<li>假设任意时刻的观测只依赖于该时刻的马尔科夫链的状态，与其他观测和状态无关。（<strong>观测独立性假设</strong>）</li>
</ol>
<h3 id="一个关于感冒的实例"><a href="#一个关于感冒的实例" class="headerlink" title="一个关于感冒的实例"></a>一个关于感冒的实例</h3><p>定义讲完了，举个实例，参考hankcs和知乎上的<strong>感冒预测</strong>的例子（实际上都是来自wikipidia: <a href="https://en.wikipedia.org/wiki/Viterbi_algorithm#Example">https://en.wikipedia.org/wiki/Viterbi_algorithm#Example</a> ），这里我用最简单的语言去描述。</p>
<p>假设你是一个医生，眼前有个病人，你的任务是确定他是否得了感冒。</p>
<ul>
<li>首先，病人的状态($Q$)只有两种：{感冒，没有感冒}。</li>
<li>然后，病人的感觉（观测$V$）有三种：{正常，冷，头晕}。</li>
<li>手头有病人的病例，你可以从病例的第一天确定$π$（初始状态概率向量）；</li>
<li>然后根据其他病例信息，确定$A$（状态转移矩阵）也就是病人某天是否感冒和他第二天是否感冒的关系；</li>
<li>还可以确定$B$（观测概率矩阵）也就是病人某天是什么感觉和他那天是否感冒的关系。</li>
</ul>
<p><img src="/machine_learning/HMM/hmm.jpg"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对应状态集合Q</span></span><br><span class="line">states = (<span class="string">&#x27;Healthy&#x27;</span>, <span class="string">&#x27;Fever&#x27;</span>)</span><br><span class="line"><span class="comment"># 对应观测集合V</span></span><br><span class="line">observations = (<span class="string">&#x27;normal&#x27;</span>, <span class="string">&#x27;cold&#x27;</span>, <span class="string">&#x27;dizzy&#x27;</span>)</span><br><span class="line"><span class="comment"># 初始状态概率向量π</span></span><br><span class="line">start_probability = &#123;<span class="string">&#x27;Healthy&#x27;</span>: <span class="number">0.6</span>, <span class="string">&#x27;Fever&#x27;</span>: <span class="number">0.4</span>&#125;</span><br><span class="line"><span class="comment"># 状态转移矩阵A</span></span><br><span class="line">transition_probability = &#123;</span><br><span class="line">    <span class="string">&#x27;Healthy&#x27;</span>: &#123;<span class="string">&#x27;Healthy&#x27;</span>: <span class="number">0.7</span>, <span class="string">&#x27;Fever&#x27;</span>: <span class="number">0.3</span>&#125;,</span><br><span class="line">    <span class="string">&#x27;Fever&#x27;</span>: &#123;<span class="string">&#x27;Healthy&#x27;</span>: <span class="number">0.4</span>, <span class="string">&#x27;Fever&#x27;</span>: <span class="number">0.6</span>&#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 观测概率矩阵B</span></span><br><span class="line">emission_probability = &#123;</span><br><span class="line">    <span class="string">&#x27;Healthy&#x27;</span>: &#123;<span class="string">&#x27;normal&#x27;</span>: <span class="number">0.5</span>, <span class="string">&#x27;cold&#x27;</span>: <span class="number">0.4</span>, <span class="string">&#x27;dizzy&#x27;</span>: <span class="number">0.1</span>&#125;,</span><br><span class="line">    <span class="string">&#x27;Fever&#x27;</span>: &#123;<span class="string">&#x27;normal&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;cold&#x27;</span>: <span class="number">0.3</span>, <span class="string">&#x27;dizzy&#x27;</span>: <span class="number">0.6</span>&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生成观测序列和状态序列    </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">simulate</span>(<span class="params">T</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">draw_from</span>(<span class="params">probs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        1.np.random.multinomial:</span></span><br><span class="line"><span class="string">        按照多项式分布，生成数据</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; np.random.multinomial(20, [1/6.]*6, size=2)</span></span><br><span class="line"><span class="string">                array([[3, 4, 3, 3, 4, 3],</span></span><br><span class="line"><span class="string">                       [2, 4, 3, 4, 0, 7]])</span></span><br><span class="line"><span class="string">         For the first run, we threw 3 times 1, 4 times 2, etc.  </span></span><br><span class="line"><span class="string">         For the second, we threw 2 times 1, 4 times 2, etc.</span></span><br><span class="line"><span class="string">        2.np.where:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; x = np.arange(9.).reshape(3, 3)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; np.where( x &gt; 5 )</span></span><br><span class="line"><span class="string">        (array([2, 2, 2]), array([0, 1, 2]))</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> np.where(np.random.multinomial(<span class="number">1</span>,probs) == <span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    observations = np.zeros(T, dtype=<span class="built_in">int</span>)</span><br><span class="line">    states = np.zeros(T, dtype=<span class="built_in">int</span>)</span><br><span class="line">    states[<span class="number">0</span>] = draw_from(pi)</span><br><span class="line">    observations[<span class="number">0</span>] = draw_from(B[states[<span class="number">0</span>],:])</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, T):</span><br><span class="line">        states[t] = draw_from(A[states[t-<span class="number">1</span>],:])</span><br><span class="line">        observations[t] = draw_from(B[states[t],:])</span><br><span class="line">    <span class="keyword">return</span> observations, states</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_index_map</span>(<span class="params">lables</span>):</span><br><span class="line">    id2label = &#123;&#125;</span><br><span class="line">    label2id = &#123;&#125;</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> lables:</span><br><span class="line">        id2label[i] = l</span><br><span class="line">        label2id[l] = i</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> id2label, label2id</span><br><span class="line"> </span><br><span class="line">states_id2label, states_label2id = generate_index_map(states)</span><br><span class="line">observations_id2label, observations_label2id = generate_index_map(observations)</span><br><span class="line"><span class="built_in">print</span>(states_id2label, states_label2id)</span><br><span class="line"><span class="built_in">print</span>(observations_id2label, observations_label2id)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;0: &#x27;Healthy&#x27;, 1: &#x27;Fever&#x27;&#125; &#123;&#x27;Healthy&#x27;: 0, &#x27;Fever&#x27;: 1&#125;</span><br><span class="line">&#123;0: &#x27;normal&#x27;, 1: &#x27;cold&#x27;, 2: &#x27;dizzy&#x27;&#125; &#123;&#x27;normal&#x27;: 0, &#x27;cold&#x27;: 1, &#x27;dizzy&#x27;: 2&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">convert_map_to_vector</span>(<span class="params">map_, label2id</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将概率向量从dict转换成一维array&quot;&quot;&quot;</span></span><br><span class="line">    v = np.zeros(<span class="built_in">len</span>(map_), dtype=<span class="built_in">float</span>)</span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> map_:</span><br><span class="line">        v[label2id[e]] = map_[e]</span><br><span class="line">    <span class="keyword">return</span> v</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convert_map_to_matrix</span>(<span class="params">map_, label2id1, label2id2</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将概率转移矩阵从dict转换成矩阵&quot;&quot;&quot;</span></span><br><span class="line">    m = np.zeros((<span class="built_in">len</span>(label2id1), <span class="built_in">len</span>(label2id2)), dtype=<span class="built_in">float</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> map_:</span><br><span class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> map_[line]:</span><br><span class="line">            m[label2id1[line]][label2id2[col]] = map_[line][col]</span><br><span class="line">    <span class="keyword">return</span> m</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A = convert_map_to_matrix(transition_probability, states_label2id, states_label2id)</span><br><span class="line"><span class="built_in">print</span>(A)</span><br><span class="line">B = convert_map_to_matrix(emission_probability, states_label2id, observations_label2id)</span><br><span class="line"><span class="built_in">print</span>(B)</span><br><span class="line">observations_index = [observations_label2id[o] <span class="keyword">for</span> o <span class="keyword">in</span> observations]</span><br><span class="line">pi = convert_map_to_vector(start_probability, states_label2id)</span><br><span class="line"><span class="built_in">print</span>(pi)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[[ 0.7  0.3]</span><br><span class="line"> [ 0.4  0.6]]</span><br><span class="line">[[ 0.5  0.4  0.1]</span><br><span class="line"> [ 0.1  0.3  0.6]]</span><br><span class="line">[ 0.6  0.4]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成模拟数据</span></span><br><span class="line">observations_data, states_data = simulate(<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(observations_data)</span><br><span class="line"><span class="built_in">print</span>(states_data)</span><br><span class="line"><span class="comment"># 相应的label</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;病人的状态: &quot;</span>, [states_id2label[index] <span class="keyword">for</span> index <span class="keyword">in</span> states_data])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;病人的观测: &quot;</span>, [observations_id2label[index] <span class="keyword">for</span> index <span class="keyword">in</span> observations_data])</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[0 0 1 1 2 1 2 2 2 0]</span><br><span class="line">[0 0 0 0 1 1 1 1 1 0]</span><br><span class="line">病人的状态:  [&#x27;Healthy&#x27;, &#x27;Healthy&#x27;, &#x27;Healthy&#x27;, &#x27;Healthy&#x27;, &#x27;Fever&#x27;, &#x27;Fever&#x27;, &#x27;Fever&#x27;, &#x27;Fever&#x27;, &#x27;Fever&#x27;, &#x27;Healthy&#x27;]</span><br><span class="line">病人的观测:  [&#x27;normal&#x27;, &#x27;normal&#x27;, &#x27;cold&#x27;, &#x27;cold&#x27;, &#x27;dizzy&#x27;, &#x27;cold&#x27;, &#x27;dizzy&#x27;, &#x27;dizzy&#x27;, &#x27;dizzy&#x27;, &#x27;normal&#x27;]</span><br></pre></td></tr></table></figure>

<h2 id="2-HMM的三个问题"><a href="#2-HMM的三个问题" class="headerlink" title="2.HMM的三个问题"></a>2.HMM的三个问题</h2><p>HMM在实际应用中，一般会遇上三种问题：</p>
<ul>
<li>1.<strong>概率计算问题</strong>：给定模型$λ&#x3D;(A,B,π)$<br>和观测序列$O&#x3D;{o_1,o_2,…,o_T}$，计算在模型$λ$下观测序列$O$出现的概率$P(O|λ)$。</li>
<li>2.<strong>学习问题</strong>：已知观测序列$O&#x3D;{o_1,o_2,…,o_T}$，估计模型$λ&#x3D;(A,B,π)$，使$P(O|λ)$最大。即用<strong>极大似然法</strong>的方法估计参数。</li>
<li>3.<strong>预测问题</strong>（也称为解码（decoding）问题）：已知观测序列$O&#x3D;{o_1,o_2,…,o_T}$ 和模型$λ&#x3D;(A,B,π)$，求给定观测序列条件概率$P(I|O)$最大的状态序列$I&#x3D;(i_1,i_2,…,i_T)$，即给定观测序列，求最有可能的对应的<strong>状态序列</strong>。</li>
</ul>
<p>回到刚才的例子，这三个问题就是：</p>
<ul>
<li>1.<strong>概率计算问题</strong>：如果给定模型参数，病人某一系列观测的症状出现的概率。</li>
<li>2.<strong>学习问题</strong>：根据病人某一些列观测的症状，学习模型参数。</li>
<li>3.<strong>预测问题</strong>：根据学到的模型，预测病人这几天是不是有感冒。</li>
</ul>
<h3 id="2-1-概率计算问题"><a href="#2-1-概率计算问题" class="headerlink" title="2.1 概率计算问题"></a>2.1 概率计算问题</h3><p>概率计算问题计算的是：在模型$λ$下观测序列$O$出现的概率$P(O|λ)$。</p>
<p><strong>直接计算</strong>：</p>
<p>对于状态序列$I&#x3D;(i_1,i_2, …,i_T)$的概率是：$P(I|\lambda)&#x3D;\pi_{i_1}a_{i_1i_2}a_{i_2i_3}…a_{i_{T-1}i_T}$。</p>
<p>对上面这种状态序列，产生观测序列$O&#x3D;(o_1, o_2, …,o_T)$的概率是$P(O|I,\lambda)&#x3D;b_{i_1}(o_1)b_{i_2}(o_2)…b_{i_T}(o_{T})$。</p>
<p>$I$和$O$的<strong>联合概率</strong>为:<br>$$P(O,I|\lambda)&#x3D;P(O|I,\lambda)P(I|\lambda)&#x3D;\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)…a_{i_{T-1}i_T}b_{i_T}(o_{T})$$</p>
<p>对所有可能的$I$求和，得到:<br>$$P(O|λ)&#x3D;\sum_IP(O,I|\lambda)&#x3D;\sum_{i_1,…,i_T}\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)…a_{i_{T-1}i_T}b_{i_T}(o_{T})$$<br>如果直接计算，时间复杂度太高，是$O(TN^T)$。</p>
<p><strong>前向算法（或者后向算法）</strong>：</p>
<p>首先引入<strong>前向概率</strong>：</p>
<p>给定模型$λ$，定义到时刻$t$部分观测序列为$o_1,o_2,…,o_t$ 且状态为$q_i$ 的概率为前向概率。记作：</p>
<p>$$α_t(i)&#x3D;P(o_1,o_2,…,o_t,i_t&#x3D;q_i|λ)$$</p>
<p>用感冒例子描述就是：某一天是否感冒以及这天和这天之前所有的观测症状的联合概率。</p>
<p>后向概率定义类似。</p>
<p><strong>前向算法</strong></p>
<p>输入：隐马模型$λ$，观测序列$O$;<br>输出：观测序列概率$P(O|λ)$.</p>
<ul>
<li>初值$(t&#x3D;1)$，$α_1(i)&#x3D;P(o_1,i_1&#x3D;q_1|λ)&#x3D;π_ib_i(o_1)$，$i&#x3D;1,2,…,N $</li>
<li>递推：对$t&#x3D;1,2,…,N$，$α_{t+1}(i)&#x3D;[\sum^N_{j&#x3D;1}α_t(j)a_{ji}]b_i(o_{t+1})$</li>
<li>终结：$P(O|λ)&#x3D;\sum^N_{i&#x3D;1}α_T(i)$</li>
</ul>
<p><strong>前向算法理解：</strong></p>
<p>前向算法使用<strong>前向概率</strong>的概念，记录每个时间下的前向概率，使得在递推计算下一个前向概率时，只需要上一个时间点的所有前向概率即可。原理上也是用空间换时间。这样的**时间复杂度是$O(N^2T)$**。</p>
<p><img src="/machine_learning/HMM/forward.png"></p>
<p>前向算法&#x2F;后向算法python实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">obs_seq</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;前向算法&quot;&quot;&quot;</span></span><br><span class="line">    N = A.shape[<span class="number">0</span>]</span><br><span class="line">    T = <span class="built_in">len</span>(obs_seq)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># F保存前向概率矩阵</span></span><br><span class="line">    F = np.zeros((N,T))</span><br><span class="line">    F[:,<span class="number">0</span>] = pi * B[:, obs_seq[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, T):</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">            F[n,t] = np.dot(F[:,t-<span class="number">1</span>], (A[:,n])) * B[n, obs_seq[t]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">obs_seq</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;后向算法&quot;&quot;&quot;</span></span><br><span class="line">    N = A.shape[<span class="number">0</span>]</span><br><span class="line">    T = <span class="built_in">len</span>(obs_seq)</span><br><span class="line">    <span class="comment"># X保存后向概率矩阵</span></span><br><span class="line">    X = np.zeros((N,T))</span><br><span class="line">    X[:,-<span class="number">1</span>:] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(T-<span class="number">1</span>)):</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">            X[n,t] = np.<span class="built_in">sum</span>(X[:,t+<span class="number">1</span>] * A[n,:] * B[:, obs_seq[t+<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>

<h3 id="2-2学习问题"><a href="#2-2学习问题" class="headerlink" title="2.2学习问题"></a>2.2学习问题</h3><p>学习问题我们这里只关注非监督的学习算法，有监督的学习算法在有标注数据的前提下，使用<strong>极大似然估计法</strong>可以很方便地估计模型参数。</p>
<p>非监督的情况，也就是我们只有一堆观测数据，对应到感冒预测的例子，即，我们只知道病人之前的几天是什么感受，但是不知道他之前是否被确认为感冒。</p>
<p>在这种情况下，我们可以使用<strong>EM算法</strong>，将<strong>状态变量视作隐变量</strong>。使用EM算法学习HMM参数的算法称为<strong>Baum-Weich算法</strong>。</p>
<p>模型表达式：<br>$$P(O|λ)&#x3D;\sum_IP(O|I,λ)P(I|λ)$$</p>
<p><strong>Baum-Weich算法</strong>：</p>
<p>(1). 确定完全数据的对数似然函数<br>完全数据是$(O,I)&#x3D;(o_1,o_2,…,o_T,i_1,…,i_T)$</p>
<p>完全数据的对数似然函数是：$logP(O,I|λ)$。</p>
<p>(2). EM算法的E步：</p>
<p>$$Q(λ,\hatλ)&#x3D;\sum_IlogP(O,I|λ)P(O,I|\hatλ)$$<br>注意，这里忽略了对于$λ$而言是常数因子的$\frac{1}{P(O|\hatλ)}$</p>
<p>其中，$\hatλ$是隐马尔科夫模型参数的当前估计值，λ是要极大化的因马尔科夫模型参数。</p>
<p>又有：<br>$$P(O,I|λ)&#x3D;π_{i_1}b_{i_1}(o_1)a_{i_1,i_2}b_{i_2}(o_2)…a_{i_T-1,i_T}b_{i_T}(o_T)$$</p>
<p>于是$Q(λ,\hatλ)$可以写成：<br>$$Q(λ,\hatλ)&#x3D;\sum_Ilogπ_{i_1}P(O,I|\hatλ)+\sum_I(\sum^{T-1}_{t&#x3D;1}loga_{i_t-1,i_t})P(O,I|\hatλ)+\sum_I(\sum^{T-1}_{t&#x3D;1}logb_{i_t}(o_t))P(O,I|\hatλ)$$</p>
<p>(3). EM算法的M步：</p>
<p>极大化Q函数$Q(λ,\hatλ)$ 求模型参数$A，B，π$。</p>
<p>应用拉格朗日乘子法对各参数求偏导，解得<strong>Baum-Weich模型参数估计公式</strong>：</p>
<p>$$a_{ij}&#x3D;\frac{\sum_{t&#x3D;1}^{T-1}ξ_t(i,j)}{\sum_{t&#x3D;1}^{T-1}γ_t(i)}$$</p>
<p>$$b_j(k)&#x3D;\frac{\sum^T_{t&#x3D;1,o_t&#x3D;v_k}γ_t(j)}{\sum_{t&#x3D;1}^{T}γ_t(j)}$$</p>
<p>$$π_i&#x3D;γ_1(i)$$</p>
<p>其中$γ_t(i)$和$ξ_t(i,j)$是：</p>
<p>$$γ_t(i)&#x3D;P(i_t&#x3D;q_i|O,λ)$$</p>
<p>$$&#x3D;\frac{P(i_t&#x3D;q_i,O|λ)}{P(O|λ)}$$</p>
<p>$$&#x3D;\frac{α_t(i)β_t(i)}{\sum_{j&#x3D;1}^Nα_t(j)β_t(j)}$$</p>
<p>读作gamma，即，<strong>给定模型参数和所有观测，时刻$t$处于状态$q_i$的概率</strong>。</p>
<p>$$ξ_t(i,j)$$</p>
<p>$$&#x3D;P(i_t&#x3D;q_i,i_{i+1}&#x3D;q_j|O,λ)$$</p>
<p>$$&#x3D;\frac{P(i_t&#x3D;q_i,i_{i+1}&#x3D;q_j,O|λ)}{P(O|λ)}$$</p>
<p>$$&#x3D;\frac{P(i_t&#x3D;q_i,i_{i+1}&#x3D;q_j,O|λ)}{\sum_{i&#x3D;1}^N\sum_{j&#x3D;1}^NP(i_t&#x3D;q_i,i_{i+1}&#x3D;q_j,O|λ)}$$</p>
<p>读作xi，即，<strong>给定模型参数和所有观测，时刻$t$处于状态$q_i$且时刻$t+1$处于状态$q_j$的概率</strong>。</p>
<p>带入$P(i_t&#x3D;q_i,i_{i+1}&#x3D;q_j,O|λ)&#x3D;α_t(i)a_{ij}b_j(o_{t+1})β_{t+1}(j)$</p>
<p>得到：$ξ_t(i,j)&#x3D;\frac{α_t(i)a_{ij}b_j(o_{t+1})β_{t+1}(j)}{\sum_{i&#x3D;1}^N\sum_{j&#x3D;1}^Nα_t(i)a_{ij}b_j(o_{t+1})β_{t+1}(j)}$</p>
<p><strong>Baum-Weich算法</strong>的python实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">baum_welch_train</span>(<span class="params">observations, A, B, pi, criterion=<span class="number">0.05</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;无监督学习算法——Baum-Weich算法&quot;&quot;&quot;</span></span><br><span class="line">    n_states = A.shape[<span class="number">0</span>]</span><br><span class="line">    n_samples = <span class="built_in">len</span>(observations)</span><br><span class="line"></span><br><span class="line">    done = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">        <span class="comment"># alpha_t(i) = P(O_1 O_2 ... O_t, q_t = S_i | hmm)</span></span><br><span class="line">        <span class="comment"># Initialize alpha</span></span><br><span class="line">        alpha = forward(observations)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># beta_t(i) = P(O_t+1 O_t+2 ... O_T | q_t = S_i , hmm)</span></span><br><span class="line">        <span class="comment"># Initialize beta</span></span><br><span class="line">        beta = backward(observations)</span><br><span class="line">        <span class="comment"># ξ_t(i,j)=P(i_t=q_i,i_&#123;i+1&#125;=q_j|O,λ)</span></span><br><span class="line">        xi = np.zeros((n_states,n_states,n_samples-<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(n_samples-<span class="number">1</span>):</span><br><span class="line">            denom = np.dot(np.dot(alpha[:,t].T, A) * B[:,observations[t+<span class="number">1</span>]].T, beta[:,t+<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_states):</span><br><span class="line">                numer = alpha[i,t] * A[i,:] * B[:,observations[t+<span class="number">1</span>]].T * beta[:,t+<span class="number">1</span>].T</span><br><span class="line">                xi[i,:,t] = numer / denom</span><br><span class="line"></span><br><span class="line">        <span class="comment"># γ_t(i)：gamma_t(i) = P(q_t = S_i | O, hmm)</span></span><br><span class="line">        gamma = np.<span class="built_in">sum</span>(xi,axis=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># Need final gamma element for new B</span></span><br><span class="line">        <span class="comment"># xi的第三维长度n_samples-1，少一个，所以gamma要计算最后一个</span></span><br><span class="line">        prod =  (alpha[:,n_samples-<span class="number">1</span>] * beta[:,n_samples-<span class="number">1</span>]).reshape((-<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        gamma = np.hstack((gamma,  prod / np.<span class="built_in">sum</span>(prod))) <span class="comment">#append one more to gamma!!!</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新模型参数</span></span><br><span class="line">        newpi = gamma[:,<span class="number">0</span>]</span><br><span class="line">        newA = np.<span class="built_in">sum</span>(xi,<span class="number">2</span>) / np.<span class="built_in">sum</span>(gamma[:,:-<span class="number">1</span>],axis=<span class="number">1</span>).reshape((-<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        newB = np.copy(B)</span><br><span class="line">        num_levels = B.shape[<span class="number">1</span>]</span><br><span class="line">        sumgamma = np.<span class="built_in">sum</span>(gamma,axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> lev <span class="keyword">in</span> <span class="built_in">range</span>(num_levels):</span><br><span class="line">            mask = observations == lev</span><br><span class="line">            newB[:,lev] = np.<span class="built_in">sum</span>(gamma[:,mask],axis=<span class="number">1</span>) / sumgamma</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 检查是否满足阈值</span></span><br><span class="line">        <span class="keyword">if</span> np.<span class="built_in">max</span>(<span class="built_in">abs</span>(pi - newpi)) &lt; criterion <span class="keyword">and</span> \</span><br><span class="line">                        np.<span class="built_in">max</span>(<span class="built_in">abs</span>(A - newA)) &lt; criterion <span class="keyword">and</span> \</span><br><span class="line">                        np.<span class="built_in">max</span>(<span class="built_in">abs</span>(B - newB)) &lt; criterion:</span><br><span class="line">            done = <span class="number">1</span></span><br><span class="line">        A[:], B[:], pi[:] = newA, newB, newpi</span><br><span class="line">    <span class="keyword">return</span> newA, newB, newpi</span><br></pre></td></tr></table></figure>

<p>回到预测感冒的问题，下面我们先自己建立一个HMM模型，再模拟出一个观测序列和一个状态序列。</p>
<p>然后，只用观测序列去学习模型，获得模型参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">A = np.array([[<span class="number">0.5</span>, <span class="number">0.5</span>],[<span class="number">0.5</span>, <span class="number">0.5</span>]])</span><br><span class="line">B = np.array([[<span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>],[<span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>]])</span><br><span class="line">pi = np.array([<span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line"></span><br><span class="line">observations_data, states_data = simulate(<span class="number">100</span>)</span><br><span class="line">newA, newB, newpi = baum_welch_train(observations_data, A, B, pi)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;newA: &quot;</span>, newA)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;newB: &quot;</span>, newB)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;newpi: &quot;</span>, newpi)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">newA:  [[ 0.5  0.5]</span><br><span class="line"> [ 0.5  0.5]]</span><br><span class="line">newB:  [[ 0.28  0.32  0.4 ]</span><br><span class="line"> [ 0.28  0.32  0.4 ]]</span><br><span class="line">newpi:  [ 0.5  0.5]</span><br></pre></td></tr></table></figure>

<h3 id="2-3预测问题"><a href="#2-3预测问题" class="headerlink" title="2.3预测问题"></a>2.3预测问题</h3><p>考虑到预测问题是求给定观测序列条件概率$P(I|O)$最大的状态序列$I&#x3D;(i_1,i_2,…,i_T)$，类比这个问题和最短路问题：<br>我们可以把求$P(I|O)$的最大值类比成求节点间距离的最小值，于是考虑<strong>类似于动态规划的viterbi算法</strong>。</p>
<p>**首先导入两个变量$δ$和$ψ$**：</p>
<p>定义<strong>在时刻$t$状态为$i$的所有单个路径$(i_1,i_2,i_3,…,i_t)$中概率最大值</strong>为(这里考虑$P(I,O)$便于计算，因为给定的$P(O)$,$P(I|O)$正比于$P(I,O)$):</p>
<p>$$δ_t(i)&#x3D;max_{i_1,i_2,…,i_t-1}P(i_t&#x3D;i,i_{t-1},…,i_1,o_t,o_{t-1},…,o_1|λ)$$</p>
<p>读作delta，其中，$i&#x3D;1,2,…,N$</p>
<p>得到其递推公式：</p>
<p>$$δ_t(i)&#x3D;max_{1≤j≤N}[δ_{t-1}(j)a_{ji}]b_i(o_1)$$</p>
<p>定义<strong>在时刻$t$状态为$i$的所有单个路径$(i_1,i_2,i_3,…,i_{t-1},i)$中概率最大的路径的第$t-1$个结点</strong>为</p>
<p>$$ψ_t(i)&#x3D;argmax_{1≤j≤N}[δ_{t-1}(j)a_{ji}]$$</p>
<p>读作psi，其中，$i&#x3D;1,2,…,N$</p>
<p>下面介绍维特比算法。<br><strong>维特比（viterbi）算法</strong>（动态规划）：</p>
<p>输入：模型$λ&#x3D;(A,B,π)$和观测$O&#x3D;(o_1,o_2,…,o_T)$<br>输出：最优路径$I^*&#x3D;(i^*_1,i^*_2,…,i^*_T)$</p>
<p>(1).初始化：<br>$$δ_1(i)&#x3D;π_ib_i(o_1)$$<br>$$ψ_1(i)&#x3D;0$$</p>
<p>(2).<strong>递推。</strong>对$t&#x3D;2,3,…,T$<br>$$δ_t(i)&#x3D;max_{1≤j≤N}[δ_{t-1}(j)a_{ji}]b_i(o_t)$$<br>$$ψ_t(i)&#x3D;argmax_{1≤j≤N}[δ_{t-1}(j)a_{ji}]$$</p>
<p>(3).终止：<br>$$P^*&#x3D;max_{1≤i≤N}δ_T(i)$$</p>
<p>$$i^*_T&#x3D;argmax_{1≤i≤N}δ_T(i)$$</p>
<p>(4).最优路径回溯，对$t&#x3D;T-1,T-2,…,1$<br>$$i^*_t&#x3D;ψ_{t+1}(i^*_{t+1})$$</p>
<p>求得最优路径$I^*&#x3D;(i_1^*,i_2^*,…,i_T^*)$</p>
<p><strong>注：上面的$b_i(o_t)$和$ψ_{t+1}(i^*_{t+1})$的括号，并不是函数，而是类似于数组取下标的操作。</strong></p>
<p>viterbi算法python实现（<strong>V对应δ，prev对应ψ</strong>）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">viterbi</span>(<span class="params">obs_seq, A, B, pi</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    V : numpy.ndarray</span></span><br><span class="line"><span class="string">        V [s][t] = Maximum probability of an observation sequence ending</span></span><br><span class="line"><span class="string">                   at time &#x27;t&#x27; with final state &#x27;s&#x27;</span></span><br><span class="line"><span class="string">    prev : numpy.ndarray</span></span><br><span class="line"><span class="string">        Contains a pointer to the previous state at t-1 that maximizes</span></span><br><span class="line"><span class="string">        V[state][t]</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    V对应δ，prev对应ψ</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    N = A.shape[<span class="number">0</span>]</span><br><span class="line">    T = <span class="built_in">len</span>(obs_seq)</span><br><span class="line">    prev = np.zeros((T - <span class="number">1</span>, N), dtype=<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># DP matrix containing max likelihood of state at a given time</span></span><br><span class="line">    V = np.zeros((N, T))</span><br><span class="line">    V[:,<span class="number">0</span>] = pi * B[:,obs_seq[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, T):</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">            seq_probs = V[:,t-<span class="number">1</span>] * A[:,n] * B[n, obs_seq[t]]</span><br><span class="line">            prev[t-<span class="number">1</span>,n] = np.argmax(seq_probs)</span><br><span class="line">            V[n,t] = np.<span class="built_in">max</span>(seq_probs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> V, prev</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_viterbi_path</span>(<span class="params">prev, last_state</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns a state path ending in last_state in reverse order.</span></span><br><span class="line"><span class="string">    最优路径回溯</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    T = <span class="built_in">len</span>(prev)</span><br><span class="line">    <span class="keyword">yield</span>(last_state)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(T-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">yield</span>(prev[i, last_state])</span><br><span class="line">        last_state = prev[i, last_state]</span><br><span class="line">        </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">observation_prob</span>(<span class="params">obs_seq</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; P( entire observation sequence | A, B, pi ) &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(forward(obs_seq)[:,-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">state_path</span>(<span class="params">obs_seq, A, B, pi</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    V[last_state, -1] : float</span></span><br><span class="line"><span class="string">        Probability of the optimal state path</span></span><br><span class="line"><span class="string">    path : list(int)</span></span><br><span class="line"><span class="string">        Optimal state path for the observation sequence</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    V, prev = viterbi(obs_seq, A, B, pi)</span><br><span class="line">    <span class="comment"># Build state path with greatest probability</span></span><br><span class="line">    last_state = np.argmax(V[:,-<span class="number">1</span>])</span><br><span class="line">    path = <span class="built_in">list</span>(build_viterbi_path(prev, last_state))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> V[last_state,-<span class="number">1</span>], <span class="built_in">reversed</span>(path)</span><br></pre></td></tr></table></figure>

<p>继续感冒预测的例子，根据刚才学得的模型参数，再去预测状态序列，观测准确率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">states_out = state_path(observations_data, newA, newB, newpi)[<span class="number">1</span>]</span><br><span class="line">p = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> states_data:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">next</span>(states_out) == s: </span><br><span class="line">        p += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(p / <span class="built_in">len</span>(states_data))</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.54</span><br></pre></td></tr></table></figure>

<p>因为是随机生成的样本，因此准确率较低也可以理解。</p>
<p>使用Viterbi算法计算病人的病情以及相应的概率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">A = convert_map_to_matrix(transition_probability, states_label2id, states_label2id)</span><br><span class="line">B = convert_map_to_matrix(emission_probability, states_label2id, observations_label2id)</span><br><span class="line">observations_index = [observations_label2id[o] <span class="keyword">for</span> o <span class="keyword">in</span> observations]</span><br><span class="line">pi = convert_map_to_vector(start_probability, states_label2id)</span><br><span class="line">V, p = viterbi(observations_index, newA, newB, newpi)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; &quot;</span> * <span class="number">7</span>, <span class="string">&quot; &quot;</span>.join((<span class="string">&quot;%10s&quot;</span> % observations_id2label[i]) <span class="keyword">for</span> i <span class="keyword">in</span> observations_index))</span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">2</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%7s: &quot;</span> % states_id2label[s] + <span class="string">&quot; &quot;</span>.join(<span class="string">&quot;%10s&quot;</span> % (<span class="string">&quot;%f&quot;</span> % v) <span class="keyword">for</span> v <span class="keyword">in</span> V[s]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nThe most possible states and probability are:&#x27;</span>)</span><br><span class="line">p, ss = state_path(observations_index, newA, newB, newpi)</span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> ss:</span><br><span class="line">    <span class="built_in">print</span>(states_id2label[s])</span><br><span class="line"><span class="built_in">print</span>(p)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">            normal       cold      dizzy</span><br><span class="line">Healthy:   0.140000   0.022400   0.004480</span><br><span class="line">  Fever:   0.140000   0.022400   0.004480</span><br><span class="line"></span><br><span class="line">The most possible states and probability are:</span><br><span class="line">Healthy</span><br><span class="line">Healthy</span><br><span class="line">Healthy</span><br><span class="line">0.00448</span><br></pre></td></tr></table></figure>

<h2 id="3-完整代码"><a href="#3-完整代码" class="headerlink" title="3.完整代码"></a>3.完整代码</h2><p>代码主要参考<a href="http://www.hankcs.com/ml/hidden-markov-model.html">Hankcs</a>的博客，hankcs参考的是<a href="http://www.cs.colostate.edu/~anderson/cs440/index.html/doku.php?id=notes:hmm2">colostate大学的教学代码</a>。</p>
<p>完整的隐马尔科夫用类包装的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">HMM</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Order 1 Hidden Markov Model</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">    Attributes</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    A : numpy.ndarray</span></span><br><span class="line"><span class="string">        State transition probability matrix</span></span><br><span class="line"><span class="string">    B: numpy.ndarray</span></span><br><span class="line"><span class="string">        Output emission probability matrix with shape(N, number of output types)</span></span><br><span class="line"><span class="string">    pi: numpy.ndarray</span></span><br><span class="line"><span class="string">        Initial state probablity vector</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, A, B, pi</span>):</span><br><span class="line">        <span class="variable language_">self</span>.A = A</span><br><span class="line">        <span class="variable language_">self</span>.B = B</span><br><span class="line">        <span class="variable language_">self</span>.pi = pi</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">simulate</span>(<span class="params">self, T</span>):</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">draw_from</span>(<span class="params">probs</span>):</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            1.np.random.multinomial:</span></span><br><span class="line"><span class="string">            按照多项式分布，生成数据</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt; np.random.multinomial(20, [1/6.]*6, size=2)</span></span><br><span class="line"><span class="string">                    array([[3, 4, 3, 3, 4, 3],</span></span><br><span class="line"><span class="string">                           [2, 4, 3, 4, 0, 7]])</span></span><br><span class="line"><span class="string">             For the first run, we threw 3 times 1, 4 times 2, etc.  </span></span><br><span class="line"><span class="string">             For the second, we threw 2 times 1, 4 times 2, etc.</span></span><br><span class="line"><span class="string">            2.np.where:</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt; x = np.arange(9.).reshape(3, 3)</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt; np.where( x &gt; 5 )</span></span><br><span class="line"><span class="string">            (array([2, 2, 2]), array([0, 1, 2]))</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            <span class="keyword">return</span> np.where(np.random.multinomial(<span class="number">1</span>,probs) == <span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        observations = np.zeros(T, dtype=<span class="built_in">int</span>)</span><br><span class="line">        states = np.zeros(T, dtype=<span class="built_in">int</span>)</span><br><span class="line">        states[<span class="number">0</span>] = draw_from(<span class="variable language_">self</span>.pi)</span><br><span class="line">        observations[<span class="number">0</span>] = draw_from(<span class="variable language_">self</span>.B[states[<span class="number">0</span>],:])</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, T):</span><br><span class="line">            states[t] = draw_from(<span class="variable language_">self</span>.A[states[t-<span class="number">1</span>],:])</span><br><span class="line">            observations[t] = draw_from(<span class="variable language_">self</span>.B[states[t],:])</span><br><span class="line">        <span class="keyword">return</span> observations,states</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, obs_seq</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;前向算法&quot;&quot;&quot;</span></span><br><span class="line">        N = <span class="variable language_">self</span>.A.shape[<span class="number">0</span>]</span><br><span class="line">        T = <span class="built_in">len</span>(obs_seq)</span><br><span class="line"></span><br><span class="line">        F = np.zeros((N,T))</span><br><span class="line">        F[:,<span class="number">0</span>] = <span class="variable language_">self</span>.pi * <span class="variable language_">self</span>.B[:, obs_seq[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, T):</span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">                F[n,t] = np.dot(F[:,t-<span class="number">1</span>], (<span class="variable language_">self</span>.A[:,n])) * <span class="variable language_">self</span>.B[n, obs_seq[t]]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> F</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_backward</span>(<span class="params">self, obs_seq</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;后向算法&quot;&quot;&quot;</span></span><br><span class="line">        N = <span class="variable language_">self</span>.A.shape[<span class="number">0</span>]</span><br><span class="line">        T = <span class="built_in">len</span>(obs_seq)</span><br><span class="line"></span><br><span class="line">        X = np.zeros((N,T))</span><br><span class="line">        X[:,-<span class="number">1</span>:] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(T-<span class="number">1</span>)):</span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">                X[n,t] = np.<span class="built_in">sum</span>(X[:,t+<span class="number">1</span>] * <span class="variable language_">self</span>.A[n,:] * <span class="variable language_">self</span>.B[:, obs_seq[t+<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">baum_welch_train</span>(<span class="params">self, observations, criterion=<span class="number">0.05</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;无监督学习算法——Baum-Weich算法&quot;&quot;&quot;</span></span><br><span class="line">        n_states = <span class="variable language_">self</span>.A.shape[<span class="number">0</span>]</span><br><span class="line">        n_samples = <span class="built_in">len</span>(observations)</span><br><span class="line"></span><br><span class="line">        done = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">            <span class="comment"># alpha_t(i) = P(O_1 O_2 ... O_t, q_t = S_i | hmm)</span></span><br><span class="line">            <span class="comment"># Initialize alpha</span></span><br><span class="line">            alpha = <span class="variable language_">self</span>._forward(observations)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># beta_t(i) = P(O_t+1 O_t+2 ... O_T | q_t = S_i , hmm)</span></span><br><span class="line">            <span class="comment"># Initialize beta</span></span><br><span class="line">            beta = <span class="variable language_">self</span>._backward(observations)</span><br><span class="line"></span><br><span class="line">            xi = np.zeros((n_states,n_states,n_samples-<span class="number">1</span>))</span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(n_samples-<span class="number">1</span>):</span><br><span class="line">                denom = np.dot(np.dot(alpha[:,t].T, <span class="variable language_">self</span>.A) * <span class="variable language_">self</span>.B[:,observations[t+<span class="number">1</span>]].T, beta[:,t+<span class="number">1</span>])</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_states):</span><br><span class="line">                    numer = alpha[i,t] * <span class="variable language_">self</span>.A[i,:] * <span class="variable language_">self</span>.B[:,observations[t+<span class="number">1</span>]].T * beta[:,t+<span class="number">1</span>].T</span><br><span class="line">                    xi[i,:,t] = numer / denom</span><br><span class="line"></span><br><span class="line">            <span class="comment"># gamma_t(i) = P(q_t = S_i | O, hmm)</span></span><br><span class="line">            gamma = np.<span class="built_in">sum</span>(xi,axis=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># Need final gamma element for new B</span></span><br><span class="line">            prod =  (alpha[:,n_samples-<span class="number">1</span>] * beta[:,n_samples-<span class="number">1</span>]).reshape((-<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">            gamma = np.hstack((gamma,  prod / np.<span class="built_in">sum</span>(prod))) <span class="comment">#append one more to gamma!!!</span></span><br><span class="line"></span><br><span class="line">            newpi = gamma[:,<span class="number">0</span>]</span><br><span class="line">            newA = np.<span class="built_in">sum</span>(xi,<span class="number">2</span>) / np.<span class="built_in">sum</span>(gamma[:,:-<span class="number">1</span>],axis=<span class="number">1</span>).reshape((-<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">            newB = np.copy(<span class="variable language_">self</span>.B)</span><br><span class="line"></span><br><span class="line">            num_levels = <span class="variable language_">self</span>.B.shape[<span class="number">1</span>]</span><br><span class="line">            sumgamma = np.<span class="built_in">sum</span>(gamma,axis=<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">for</span> lev <span class="keyword">in</span> <span class="built_in">range</span>(num_levels):</span><br><span class="line">                mask = observations == lev</span><br><span class="line">                newB[:,lev] = np.<span class="built_in">sum</span>(gamma[:,mask],axis=<span class="number">1</span>) / sumgamma</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> np.<span class="built_in">max</span>(<span class="built_in">abs</span>(<span class="variable language_">self</span>.pi - newpi)) &lt; criterion <span class="keyword">and</span> \</span><br><span class="line">                            np.<span class="built_in">max</span>(<span class="built_in">abs</span>(<span class="variable language_">self</span>.A - newA)) &lt; criterion <span class="keyword">and</span> \</span><br><span class="line">                            np.<span class="built_in">max</span>(<span class="built_in">abs</span>(<span class="variable language_">self</span>.B - newB)) &lt; criterion:</span><br><span class="line">                done = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="variable language_">self</span>.A[:],<span class="variable language_">self</span>.B[:],<span class="variable language_">self</span>.pi[:] = newA,newB,newpi</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">observation_prob</span>(<span class="params">self, obs_seq</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; P( entire observation sequence | A, B, pi ) &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> np.<span class="built_in">sum</span>(<span class="variable language_">self</span>._forward(obs_seq)[:,-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">state_path</span>(<span class="params">self, obs_seq</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        V[last_state, -1] : float</span></span><br><span class="line"><span class="string">            Probability of the optimal state path</span></span><br><span class="line"><span class="string">        path : list(int)</span></span><br><span class="line"><span class="string">            Optimal state path for the observation sequence</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        V, prev = <span class="variable language_">self</span>.viterbi(obs_seq)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Build state path with greatest probability</span></span><br><span class="line">        last_state = np.argmax(V[:,-<span class="number">1</span>])</span><br><span class="line">        path = <span class="built_in">list</span>(<span class="variable language_">self</span>.build_viterbi_path(prev, last_state))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> V[last_state,-<span class="number">1</span>], <span class="built_in">reversed</span>(path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">viterbi</span>(<span class="params">self, obs_seq</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        V : numpy.ndarray</span></span><br><span class="line"><span class="string">            V [s][t] = Maximum probability of an observation sequence ending</span></span><br><span class="line"><span class="string">                       at time &#x27;t&#x27; with final state &#x27;s&#x27;</span></span><br><span class="line"><span class="string">        prev : numpy.ndarray</span></span><br><span class="line"><span class="string">            Contains a pointer to the previous state at t-1 that maximizes</span></span><br><span class="line"><span class="string">            V[state][t]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        N = <span class="variable language_">self</span>.A.shape[<span class="number">0</span>]</span><br><span class="line">        T = <span class="built_in">len</span>(obs_seq)</span><br><span class="line">        prev = np.zeros((T - <span class="number">1</span>, N), dtype=<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># DP matrix containing max likelihood of state at a given time</span></span><br><span class="line">        V = np.zeros((N, T))</span><br><span class="line">        V[:,<span class="number">0</span>] = <span class="variable language_">self</span>.pi * <span class="variable language_">self</span>.B[:,obs_seq[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, T):</span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">                seq_probs = V[:,t-<span class="number">1</span>] * <span class="variable language_">self</span>.A[:,n] * <span class="variable language_">self</span>.B[n, obs_seq[t]]</span><br><span class="line">                prev[t-<span class="number">1</span>,n] = np.argmax(seq_probs)</span><br><span class="line">                V[n,t] = np.<span class="built_in">max</span>(seq_probs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> V, prev</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_viterbi_path</span>(<span class="params">self, prev, last_state</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Returns a state path ending in last_state in reverse order.&quot;&quot;&quot;</span></span><br><span class="line">        T = <span class="built_in">len</span>(prev)</span><br><span class="line">        <span class="keyword">yield</span>(last_state)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(T-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">yield</span>(prev[i, last_state])</span><br><span class="line">            last_state = prev[i, last_state]</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"># Python</a>
              <a href="/tags/NLP/" rel="tag"># NLP</a>
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/machine_learning/EM/" rel="prev" title="EM算法总结">
                  <i class="fa fa-angle-left"></i> EM算法总结
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/machine_learning/Tree_Basic/" rel="next" title="决策树基础">
                  决策树基础 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Javen Chen</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
